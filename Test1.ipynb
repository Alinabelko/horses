{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data/YNDX_160101_161231.csv\n",
      "Read done, got 131542 rows, 99752 filtered, 0 open prices adjusted\n",
      "data is None\n",
      "Reading data/YNDX_150101_151231.csv\n",
      "Read done, got 130566 rows, 104412 filtered, 0 open prices adjusted\n",
      "719: done 100 games, mean reward -0.199, mean steps 6.32, speed 436.55 f/s, eps 1.00\n",
      "1365: done 200 games, mean reward -0.218, mean steps 5.94, speed 1179.21 f/s, eps 1.00\n",
      "1979: done 300 games, mean reward -0.198, mean steps 5.70, speed 1169.90 f/s, eps 1.00\n",
      "2649: done 400 games, mean reward -0.197, mean steps 5.74, speed 1111.47 f/s, eps 1.00\n",
      "3314: done 500 games, mean reward -0.189, mean steps 5.74, speed 1101.35 f/s, eps 1.00\n",
      "3975: done 600 games, mean reward -0.193, mean steps 5.75, speed 1034.77 f/s, eps 1.00\n",
      "4653: done 700 games, mean reward -0.186, mean steps 5.77, speed 966.12 f/s, eps 1.00\n",
      "5320: done 800 games, mean reward -0.186, mean steps 5.77, speed 987.01 f/s, eps 0.99\n",
      "6064: done 900 games, mean reward -0.195, mean steps 5.85, speed 993.65 f/s, eps 0.99\n",
      "6684: done 1000 games, mean reward -0.193, mean steps 5.80, speed 901.45 f/s, eps 0.99\n",
      "7388: done 1100 games, mean reward -0.194, mean steps 5.83, speed 1023.59 f/s, eps 0.99\n",
      "8127: done 1200 games, mean reward -0.197, mean steps 5.88, speed 1004.40 f/s, eps 0.99\n",
      "8837: done 1300 games, mean reward -0.197, mean steps 5.91, speed 1056.89 f/s, eps 0.99\n",
      "9447: done 1400 games, mean reward -0.197, mean steps 5.86, speed 1168.96 f/s, eps 0.99\n",
      "Initial buffer populated, start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\_RL\\Deep-Reinforcement-Learning-Hands-On-master\\Chapter08\\lib\\common.py:100: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen/native/IndexingUtils.h:20.)\n",
      "  next_state_values[done_mask] = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10094: done 1500 games, mean reward -0.196, mean steps 5.84, speed 518.44 f/s, eps 0.99\n",
      "10816: done 1600 games, mean reward -0.202, mean steps 5.87, speed 160.53 f/s, eps 0.99\n",
      "11512: done 1700 games, mean reward -0.198, mean steps 5.88, speed 161.78 f/s, eps 0.99\n",
      "12141: done 1800 games, mean reward -0.198, mean steps 5.86, speed 162.42 f/s, eps 0.99\n",
      "12895: done 1900 games, mean reward -0.196, mean steps 5.90, speed 168.32 f/s, eps 0.99\n",
      "13638: done 2000 games, mean reward -0.198, mean steps 5.93, speed 160.94 f/s, eps 0.99\n",
      "14322: done 2100 games, mean reward -0.197, mean steps 5.93, speed 164.79 f/s, eps 0.99\n",
      "15024: done 2200 games, mean reward -0.199, mean steps 5.94, speed 168.36 f/s, eps 0.98\n",
      "15788: done 2300 games, mean reward -0.196, mean steps 5.98, speed 162.05 f/s, eps 0.98\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import ptan\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from lib import environ, data, models, common, validation\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "BARS_COUNT = 10\n",
    "TARGET_NET_SYNC = 1000\n",
    "DEFAULT_STOCKS = \"data/YNDX_160101_161231.csv\"\n",
    "DEFAULT_VAL_STOCKS = \"data/YNDX_150101_151231.csv\"\n",
    "\n",
    "GAMMA = 0.99\n",
    "\n",
    "REPLAY_SIZE = 100000\n",
    "REPLAY_INITIAL = 10000\n",
    "\n",
    "REWARD_STEPS = 2\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "STATES_TO_EVALUATE = 1000\n",
    "EVAL_EVERY_STEP = 1000\n",
    "\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_STOP = 0.1\n",
    "EPSILON_STEPS = 1000000\n",
    "\n",
    "CHECKPOINT_EVERY_STEP = 1000000\n",
    "VALIDATION_EVERY_STEP = 100000\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_in = {'cuda' : True,'data' : DEFAULT_STOCKS,'year' : None,'val' : DEFAULT_VAL_STOCKS,'r' : \"test\"}\n",
    "    device = torch.device(\"cuda\" if data_in['cuda'] else \"cpu\")\n",
    "    \n",
    "    #saves_path = os.path.join(\"/content/sample_data/saves\", data_in['r'])\n",
    "    #os.makedirs(saves_path, exist_ok=True)\n",
    "    \n",
    "    #parser = argparse.ArgumentParser()\n",
    "    #parser.add_argument(\"--cuda\", default=False, action=\"store_true\", help=\"Enable cuda\")\n",
    "    #parser.add_argument(\"--data\", default=DEFAULT_STOCKS, help=\"Stocks file or dir to train on, default=\" + DEFAULT_STOCKS)\n",
    "    #parser.add_argument(\"--year\", type=int, help=\"Year to be used for training, if specified, overrides --data option\")\n",
    "    #parser.add_argument(\"--valdata\", default=DEFAULT_VAL_STOCKS, help=\"Stocks data for validation, default=\" + DEFAULT_VAL_STOCKS)\n",
    "    #parser.add_argument(\"-r\", \"--run\", default=\"Test run\", required=True, help=\"Run name\")\n",
    "    #args = parser.parse_args()\n",
    "    #device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "    saves_path = os.path.join(\"saves\", data_in['r'])\n",
    "    os.makedirs(saves_path, exist_ok=True)\n",
    "    \n",
    "    if data_in['year'] is not None or os.path.isfile(data_in['data']):\n",
    "      if data_in['year'] is not None:\n",
    "        stock_data = data.load_year_data(data_in['year'])\n",
    "        print(\"data not None\",\"stosk =\", stock_data )\n",
    "      else:\n",
    "        stock_data = {\"YNDX\": data.load_relative(data_in['data'])}\n",
    "        print(\"data is None\")\n",
    "      env = environ.StocksEnv(stock_data, bars_count=BARS_COUNT, reset_on_close=True, state_1d=False, volumes=False)\n",
    "      env_tst = environ.StocksEnv(stock_data, bars_count=BARS_COUNT, reset_on_close=True, state_1d=False)\n",
    "    elif os.path.isdir(data_in['data']):\n",
    "      env = environ.StocksEnv.from_dir(data_in['data'], bars_count=BARS_COUNT, reset_on_close=True, state_1d=False)\n",
    "      env_tst = environ.StocksEnv.from_dir(data_in['data'], bars_count=BARS_COUNT, reset_on_close=True, state_1d=False)\n",
    "    else:\n",
    "      raise RuntimeError(\"No data to train on\")\n",
    "    \n",
    "    #if args.year is not None or os.path.isfile(args.data):\n",
    "    #    if args.year is not None:\n",
    "    #        stock_data = data.load_year_data(args.year)\n",
    "    #    else:\n",
    "    #        stock_data = {\"YNDX\": data.load_relative(args.data)}\n",
    "    #    env = environ.StocksEnv(stock_data, bars_count=BARS_COUNT, reset_on_close=True, state_1d=False, volumes=False)\n",
    "    #    env_tst = environ.StocksEnv(stock_data, bars_count=BARS_COUNT, reset_on_close=True, state_1d=False)\n",
    "    #elif os.path.isdir(args.data):\n",
    "    #    env = environ.StocksEnv.from_dir(args.data, bars_count=BARS_COUNT, reset_on_close=True, state_1d=False)\n",
    "    #    env_tst = environ.StocksEnv.from_dir(args.data, bars_count=BARS_COUNT, reset_on_close=True, state_1d=False)\n",
    "    #else:\n",
    "    #    raise RuntimeError(\"No data to train on\")\n",
    "        \n",
    "        \n",
    "    env = gym.wrappers.TimeLimit(env, max_episode_steps=1000)\n",
    "\n",
    "    val_data = {\"YNDX\": data.load_relative(data_in['val'])}\n",
    "    env_val = environ.StocksEnv(val_data, bars_count=BARS_COUNT, reset_on_close=True, state_1d=False)\n",
    "\n",
    "    writer = SummaryWriter(comment=\"-simple-\" + data_in['r'])\n",
    "    net = models.SimpleFFDQN(env.observation_space.shape[0], env.action_space.n).to(device)\n",
    "    tgt_net = ptan.agent.TargetNet(net)\n",
    "    selector = ptan.actions.EpsilonGreedyActionSelector(EPSILON_START)\n",
    "    agent = ptan.agent.DQNAgent(net, selector, device=device)\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, GAMMA, steps_count=REWARD_STEPS)\n",
    "    buffer = ptan.experience.ExperienceReplayBuffer(exp_source, REPLAY_SIZE)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # main training loop\n",
    "    step_idx = 0\n",
    "    eval_states = None\n",
    "    best_mean_val = None\n",
    "\n",
    "    with common.RewardTracker(writer, np.inf, group_rewards=100) as reward_tracker:\n",
    "        while True:\n",
    "            step_idx += 1\n",
    "            buffer.populate(1)\n",
    "            selector.epsilon = max(EPSILON_STOP, EPSILON_START - step_idx / EPSILON_STEPS)\n",
    "\n",
    "            new_rewards = exp_source.pop_rewards_steps()\n",
    "            if new_rewards:\n",
    "                reward_tracker.reward(new_rewards[0], step_idx, selector.epsilon)\n",
    "\n",
    "            if len(buffer) < REPLAY_INITIAL:\n",
    "                continue\n",
    "\n",
    "            if eval_states is None:\n",
    "                print(\"Initial buffer populated, start training\")\n",
    "                eval_states = buffer.sample(STATES_TO_EVALUATE)\n",
    "                eval_states = [np.array(transition.state, copy=False) for transition in eval_states]\n",
    "                eval_states = np.array(eval_states, copy=False)\n",
    "\n",
    "            if step_idx % EVAL_EVERY_STEP == 0:\n",
    "                mean_val = common.calc_values_of_states(eval_states, net, device=device)\n",
    "                writer.add_scalar(\"values_mean\", mean_val, step_idx)\n",
    "                if best_mean_val is None or best_mean_val < mean_val:\n",
    "                    if best_mean_val is not None:\n",
    "                        print(\"%d: Best mean value updated %.3f -> %.3f\" % (step_idx, best_mean_val, mean_val))\n",
    "                    best_mean_val = mean_val\n",
    "                    torch.save(net.state_dict(), os.path.join(saves_path, \"mean_val-%.3f.data\" % mean_val))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            batch = buffer.sample(BATCH_SIZE)\n",
    "            loss_v = common.calc_loss(batch, net, tgt_net.target_model, GAMMA ** REWARD_STEPS, device=device)\n",
    "            loss_v.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if step_idx % TARGET_NET_SYNC == 0:\n",
    "                tgt_net.sync()\n",
    "\n",
    "            if step_idx % CHECKPOINT_EVERY_STEP == 0:\n",
    "                idx = step_idx // CHECKPOINT_EVERY_STEP\n",
    "                torch.save(net.state_dict(), os.path.join(saves_path, \"checkpoint-%3d.data\" % idx))\n",
    "\n",
    "            if step_idx % VALIDATION_EVERY_STEP == 0:\n",
    "                res = validation.validation_run(env_tst, net, device=device)\n",
    "                for key, val in res.items():\n",
    "                    writer.add_scalar(key + \"_test\", val, step_idx)\n",
    "                res = validation.validation_run(env_val, net, device=device)\n",
    "                for key, val in res.items():\n",
    "                    writer.add_scalar(key + \"_val\", val, step_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
